---
title: Squid 高级优化指南/Squid Advanced Tuning Guide(续1)
author: kxn
type: post
date: 2006-01-11T16:18:23+00:00
url: /index.php/archives/58
views:
  - 4081
categories:
  - Network
  - Tech Notes

---
Squid 高级优化指南 (续1)

by kang[at]kangkang[dot]org , 转载请保留

(续上节) 

B  根据 squid 访问的模式，进行业务拆分

　进行了 Expires Header 的处理以后，squid 就真正可以起到加速的作用了，你可能也能感觉到，网站的访问速度明显加快。但是不要满足于这点成绩，查看 squid 的 snmp 统计图，通常 hit ratio 并不会太高，有 50% 就了不起了。这就是我们需要进一步优化的，我们的目标是让大部分 squid 都达到 9X% 的命中率。

  为什么 squid 命中这么低呢，这大概有两种原因。大多数的网站都是有一些页面不能够被缓存的，例如登录页面。这些页面请求也从 squid 走，成为分母的一部分，直接就降低了命中率，我们首先可以做的事情是，把这些不能够缓存的页面请求，拆分到单独一个 squid 上面，或者访问量不大的话，干脆把 apache 暴露出来。这样能够缓存的那个 squid 命中率马上上升一截。

　有人可能会说，把不能缓存的页面分拆开去，就光为了让能缓存的那个数字好看，这不是掩耳盗铃么？其实这么做是有意义的，首先就是去掉了不能缓存页面的干扰，使得我们进一步优化 squid 的依据更加准确。其次是不可缓存请求和可缓存请求之间的重要性通常是有差距的，分拆了以后，它们之间不容易互相抢占资源，不会因为下载图片的连接太多把 squid 占满，影响更重要的登录请求。第三就是可缓存内容通常是图片等页面元素,　浏览器在 load 它们的时候，对每个站点的并发连接会有控制，如果分开成不同的IP，可以多一些请求同时执行。提高少许显示速度。

  其实观察 sohu, sina 之类的页面，你会发现它们的页面也是分拆的，可以看到页面里面的图片都是指向 images.sohu.com 之类的地址，虽然它们可能和其他页面一样后台都指向同一个 apache。

　这样做完，缓存命中率大概能上升到 70%-80% 了，运气好的时候完全可以上 90%。

   另一种导致 squid 命中低的原因和这个比较类似，同样都是可缓存的内容，有的可能是软件下载站上面的大文件，有的是新闻站点上面的小图片，如果同一个 squid 对这样差别巨大的文件加速的话，会严重干扰 squid 的缓存策略，两者不能兼顾，要不就是大文件占据了 cache ，把小文件都挤出了 cache, 要不就是小文件特别多，大文件无法进入 cache, 导致大文件经常 miss 。这个比不能缓存的页面还要恶心，因此即使在服务器资源有限的情况下，也要优先拆分这两类型访问。一般来说，文件大小分界线定在 1M 左右就可以了，如果是有软件下载这样特别大的文件，可以在 4M - 10M 左右再拆分一次。对于不同访问类型的 squid, 其系统优化参数也会有所不同，这个我们后面还会讲到。

  　只要悉心按照访问模式来拆分业务，大部分起缓存作用的 squid 都可以达到很高的命中率，至少都可以到达 9X%。

(待续)

最后致谢一下 [<font color="#0b76ae">windtear</font>][1] ，这位是 squid 之王, Lord of Squid, 业务分拆是从他那里学来的。致敬。(假如有人转载请不要删除这个，否则你也太没良心了)

 [1]: http://windtear.net/